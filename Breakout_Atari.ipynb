{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dqn_Breakout_working.ipynb","provenance":[{"file_id":"1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t","timestamp":1589975937220},{"file_id":"1A75J2xFYjpJvNWXCSM1Xcty7K3WIGrud","timestamp":1542848369662}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"odNaDE1zyrL2","colab_type":"text"},"source":["# install dependancies, takes around 45 seconds\n","\n","Rendering Dependancies\n","\n"]},{"cell_type":"code","metadata":{"id":"8-AxnvAVyzQQ","colab_type":"code","colab":{}},"source":["#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8A-1LTSH88EE","colab_type":"text"},"source":["CarRacing Dependancies"]},{"cell_type":"code","metadata":{"id":"TCelFzWY9MBI","colab_type":"code","outputId":"b999bb8b-475a-4407-a5dd-11759be7ab7b","executionInfo":{"status":"ok","timestamp":1590759541842,"user_tz":-330,"elapsed":27547,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[atari] > /dev/null 2>&1\n","!pip install gym[box2d] > /dev/null 2>&1"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (47.1.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"APXSx7hg19TH","colab_type":"text"},"source":["# Imports and Helper functions\n"]},{"cell_type":"code","metadata":{"id":"fpJS38lhZhLB","colab_type":"code","outputId":"fd4f0677-ceee-4c4a-fe40-d1c8cb47e3c8","executionInfo":{"status":"ok","timestamp":1590759793808,"user_tz":-330,"elapsed":247176,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f9H47YRpX-6f","colab_type":"code","colab":{}},"source":["root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'Colab Notebooks/DQN/Breakout/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQEtc28G4niA","colab_type":"code","outputId":"04d1f57b-8555-4229-a9da-f1ebfcc3233b","executionInfo":{"status":"ok","timestamp":1590759805342,"user_tz":-330,"elapsed":2579,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f5ef81f94e0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"pdb2JwZy4jGj","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9UWeToN4r7D","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDC7CelSOed7","colab_type":"code","outputId":"1dfde699-73ef-45a6-bcda-e42f0e714ffb","executionInfo":{"status":"ok","timestamp":1590711534900,"user_tz":-330,"elapsed":11226,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"colab":{"base_uri":"https://localhost:8080/","height":567}},"source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 17658531919813942574\n",", name: \"/device:XLA_CPU:0\"\n","device_type: \"XLA_CPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 17626257282692675058\n","physical_device_desc: \"device: XLA_CPU device\"\n",", name: \"/device:XLA_GPU:0\"\n","device_type: \"XLA_GPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 9513092846313748010\n","physical_device_desc: \"device: XLA_GPU device\"\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 15701463552\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 13614745214897403017\n","physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n","]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VPwFRTuABPl-","colab_type":"text"},"source":["Car Racing DQN"]},{"cell_type":"code","metadata":{"id":"mKFMwXqPBSyr","colab_type":"code","colab":{}},"source":["import matplotlib\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import gym\n","import random\n","from collections import deque\n","import tensorflow as tf\n","import tensorflow.keras as tfk\n","import tensorflow.keras.layers as tfkl\n","import pickle\n","import time\n","import sys\n","\n","#Preprocessing\n","def gray_scale(img):\n","    return np.dot(img, [0.299, 0.587, 0.144]).astype(np.uint8)\n","\n","def down_sample(img):\n","    return img[::2, ::2]\n","\n","def crop(img, top=18, bottom=7, left=0, right=0):\n","    h, w = img.shape\n","    return img[top : h - bottom, left : w - right]\n","\n","def preprocess(state, img):\n","    img = crop(down_sample(gray_scale(img)))\n","    state = np.roll(state, -1, axis=2)\n","    state[:, :, 2] = img\n","    return state  #.reshape(1, 80, 80, 1)\n","\n","class ReplayMemory(object):\n","    def __init__(self, list, max_size):\n","        self.max_size = max_size\n","        self.cntr = 0\n","        self.mem = deque(list, maxlen = self.max_size)\n","\n","    def store(self, phi_St, action, reward, next_phi_St, done):\n","        self.mem.append((phi_St, action, reward, next_phi_St, done))\n","        self.cntr += 1\n","\n","    def sample(self, batch_size):\n","        batch_size = min(batch_size, self.cntr)\n","        return batch_size, random.sample(self.mem, batch_size)\n","\n","class DQNAgent(object):\n","    def __init__(self):\n","        self.model_id = 1\n","        self.input_shape = (80, 80, 4)\n","        self.action_count = 4\n","        self.epsilon_end = 0.01\n","        self.epsilon_step = 0.996 #0.0009 #0.0001\n","        self.no_of_episodes = 500 #no of episodes\n","        self.replay_mem_max_size = 14000 #1000000 #replay memory capacity\n","        self.batch_size = 32 #16 #do trial-error #replay batch size\n","        self.skip_frames = 0\n","        self.gamma = 0.99 #discount factor\n","        self.C = 5000 #10000\n","        self.l_rate = 0.0005 #0.00025\n","        self.discrete_action_ids = [i for i in range(self.action_count)]\n","        self.epi_score = 0\n","        self.total_score = []\n","        self.epi_loss = 0\n","        self.total_loss = []\n","        #self.val_loss = []\n","        self.skip_cntr = 0\n","        root_dir = \"/content/gdrive/My Drive/\"\n","        self.base_dir = root_dir + 'Colab Notebooks/DQN/Breakout/'\n","        self.datapath = self.base_dir + \"data/model\" + str(self.model_id) + \"/\"\n","        self.log_file = open(self.datapath + \"dqn_log.txt\", \"a+\")\n","        self.replay_mem_file = self.datapath + \"replay_mem_data.file\"\n","        self.epsilon_file = self.datapath + \"epsilon.file\"\n","        self.trained_model_file = self.datapath + \"trained_model.h5\"\n","        self.latest_weights_file = self.datapath + \"latest_weights.h5\"  #My Drive/Colab Notebooks/DQN/data/\n","\n","        try:\n","            self.replay_mem = pickle.load(open(self.replay_mem_file, \"rb\"))\n","            if len(self.replay_mem.mem) > self.replay_mem_max_size:\n","                self.replay_mem = ReplayMemory(self.replay_mem.mem, self.replay_mem_max_size)\n","            print(\"Replay Memory loaded. Length: \", len(self.replay_mem.mem))\n","        except Exception as e:\n","            self.replay_mem = ReplayMemory([], self.replay_mem_max_size)\n","            print(\"Error in loading Replay Memory. \", e)\n","\n","        try:\n","            self.epsilon = pickle.load(open(self.epsilon_file, \"rb\"))\n","            print(\"Epsilon loaded. Value: \", self.epsilon)\n","        except Exception as e:\n","            self.epsilon = 1\n","            print(\"Error in loading Epsilon. \", e)\n","\n","    def create_model(self):\n","        try:\n","            self.model = tfk.models.load_model(self.trained_model_file)\n","            print(\"Trained model loaded.\")\n","        except Exception as e:\n","            print(\"Error in loading model. \", e)\n","            self.model = tfk.models.Sequential([\n","            tfkl.Conv2D(16, (8, 8), strides = 4, activation = 'relu', input_shape=self.input_shape), #elu or relu\n","            tfkl.Conv2D(32, (4, 4), strides = 2, activation = 'relu'),\n","            tfkl.Conv2D(32, (3, 3), strides = 1, activation = 'relu'),\n","            tfkl.Flatten(),\n","            tfkl.Dense(256, activation='relu'),\n","            tfkl.Dense(self.action_count) #, activation='softmax')\n","            ])\n","\n","            #self.model.compile(optimizer=tfk.optimizers.RMSprop(learning_rate=self.l_rate, momentum=0.95), loss='mse')\n","            self.model.compile(optimizer=tfk.optimizers.Adam(learning_rate=self.l_rate), loss='mse')\n","                                                \n","            try:\n","                self.model.load_weights(self.latest_weights_file)\n","                print(\"Latest weights loaded.\")\n","            except Exception as e:\n","                print(\"Error in loading Latest weights. \", e)\n","\n","        self.new_weights = self.old_weights = self.model.get_weights()\n","\n","    def take_skip_action(self, phi_St):\n","        if self.skip_cntr % self.skip_frames != 0:\n","            next_x, reward, done, info = env.step(self.skip_action)\n","            self.epi_score += reward\n","            self.skip_cntr += 1\n","            skipped = True\n","        else:\n","            next_x = None\n","            reward = 0\n","            done = False\n","            info = None\n","            self.skip_cntr = 0\n","            skipped = False\n","        return preprocess(phi_St, next_x), reward, done, info, skipped\n","\n","    def take_e_greedy_action(self, phi_St):\n","        if random.uniform(0, 1) < self.epsilon:\n","            action = np.random.choice(self.discrete_action_ids)\n","        else:\n","            self.model.set_weights(self.new_weights)\n","            actions = self.model.predict(phi_St[np.newaxis, : ])\n","            action = np.argmax(actions)\n","           # print(actions)\n","           # print(action)\n","        self.skip_action = action\n","        next_x, reward, done, info = env.step(action)\n","        self.epi_score += reward\n","        return preprocess(phi_St, next_x), action, reward, done, info\n","    \n","    def take_predicted_action(self, phi_St):\n","        actions = self.model.predict(current_state)\n","        action = np.argmax(actions)\n","        next_x, reward, done, info = env.step(self.action)\n","        self.epi_score += reward\n","        return preprocess(phi_St, next_x), action, reward, done, info\n","\n","    def update_epsilon(self):\n","        #self.epsilon -= self.epsilon_step\n","        if self.epsilon > self.epsilon_end:\n","            self.epsilon = self.epsilon * self.epsilon_step \n","        else:\n","            self.epsilon = self.epsilon_end        \n","\n","    def store_transition(self, phi_St, action, reward, next_phi_St, done):\n","        self.replay_mem.store(phi_St, action, reward, next_phi_St, float(done))\n","\n","    def sample_transition(self):\n","        batch_size, samples = self.replay_mem.sample(self.batch_size)\n","        phi_Sts, actions, rewards, next_phi_Sts, dones = zip(*samples)\n","        phi_Sts = np.stack(phi_Sts)\n","        actions = np.stack(actions)\n","        rewards = np.stack(rewards)\n","        next_phi_Sts = np.stack(next_phi_Sts)\n","        dones = np.stack(dones)\n","        return batch_size, phi_Sts, actions, rewards, next_phi_Sts, dones\n","\n","    def optimize_loss(self, batch_size, phi_Sts, actions, rewards, next_phi_Sts, dones):\n","        self.model.set_weights(self.old_weights)\n","        phi_Sts = phi_Sts.astype(np.float32) /255.0\n","        next_phi_Sts = next_phi_Sts.astype(np.float32) / 255.0\n","        q_vals = self.model.predict(next_phi_Sts)\n","        q_vals = np.max(q_vals, axis=1)\n","        batch_list = np.arange(batch_size, dtype=np.int32)\n","        self.model.set_weights(self.new_weights)\n","        target_q_vals = self.model.predict(phi_Sts)\n","        target_q_vals[batch_list, actions] = rewards + (1 - dones) * self.gamma * q_vals\n","        self.old_weights = self.new_weights\n","        #print(\"\\ntarget \\n\", target_q_vals)\n","        #print(\"\\npredict \\n\", self.model.predict(phi_Sts))\n","        #print(\"prediction states \", phi_Sts)\n","        #print(\"prediction next states \", self.model.predict(next_phi_Sts))\n","        \n","        hist = self.model.fit(phi_Sts, target_q_vals, verbose=0)\n","        self.new_weights = self.model.get_weights()\n","        self.epi_loss += np.sum(hist.history['loss'])\n","        #self.val_loss.append(np.sum(hist.history['val_loss']))\n","\n","    def reset_target_params(self, epochs):\n","        if epochs % self.C == 0:\n","            self.old_weights = self.new_weights\n","\n","    def end_episode(self):\n","        self.total_score.append(self.epi_score)\n","        self.total_loss.append(self.epi_loss)\n","        self.epi_score = 0\n","        self.epi_loss = 0\n","\n","    def print_status(self, episode, elapsed_time):\n","        str = \"\\nepisode: {}  score: {:.5f}  loss: {:.5f}  epsilon: {:.3f}  {:.2f}s {}\"\n","        str = str.format(episode, self.epi_score, self.epi_loss, self.epsilon, \\\n","                         elapsed_time, time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n","        self.log_file.write(str)\n","        print(str)\n","\n","    def close(self):\n","        self.log_file.close()\n","\n","    def save_partial_status(self, episode):\n","        if episode % 10 == 0 or episode == self.no_of_episodes:\n","            agent.model.save(self.trained_model_file)\n","            self.log_file.close()\n","            self.log_file = open(self.datapath + \"dqn_log.txt\", \"a+\")\n","            self.model.save_weights(self.latest_weights_file)\n","            with open(self.epsilon_file, \"wb\") as f:\n","                pickle.dump(self.epsilon, f, pickle.HIGHEST_PROTOCOL)\n","           # with open(self.replay_mem_file, \"wb\") as f:\n","           #     pickle.dump(self.replay_mem, f, pickle.HIGHEST_PROTOCOL)\n","            print(\"Mem length: \", len(self.replay_mem.mem), \"Size: \", sys.getsizeof(self.replay_mem.mem))\n","    \n","    def plot(self):\n","        if len(self.total_loss) > 1:\n","            N = np.arange(0, len(self.total_loss))\n","            plt.figure()\n","            plt.plot(N, self.total_loss, label = \"train_loss\")\n","            #plt.plot(N, self.val_loss, label = \"val_loss\")\n","            plt.title(\"Training Loss\")\n","            plt.xlabel(\"Epoch #\")\n","            plt.ylabel(\"Loss\")\n","            plt.legend()\n","            plt.savefig(self.datapath + 'LossPlot.png')\n","            plt.close()\n","\n","        if len(self.total_score) > 1:\n","            N = np.arange(0, self.no_of_episodes)\n","            plt.figure()\n","            plt.plot(N, self.total_score, label = \"reward\")\n","            plt.title(\"\")\n","            plt.xlabel(\"Episode #\")\n","            plt.ylabel(\"Reward\")\n","            plt.legend()\n","            plt.savefig(self.datapath + 'RewardPlot.png')\n","            plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NckwMoBsT16T","colab_type":"text"},"source":["TRAINING"]},{"cell_type":"code","metadata":{"id":"p_S_C5uiTzA5","colab_type":"code","outputId":"4f54c1f6-d8b3-4bf2-dbfd-7dcc9772fbad","executionInfo":{"status":"error","timestamp":1590766444060,"user_tz":-330,"elapsed":6539624,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["agent = DQNAgent()\n","agent.create_model()\n","\n","epi_start = 1\n","epochs = 0\n","env = gym.make ('BreakoutDeterministic-v4') \n","#env = wrap_env(gym.make('CarRacing-v0'))\n","#print(env.observation_space, env.action_space)\n","\n","for episode in range(epi_start, agent.no_of_episodes+epi_start):\n","    start_time = time.time()\n","    #epsilon = epsilon/2\n","    done = False\n","    phi_St = preprocess(np.zeros(agent.input_shape), env.reset())\n","\n","    while not done:\n","        epochs += 1\n","\n","        if agent.skip_frames > 0 :\n","            phi_St, reward, done, info, skipped = agent.take_skip_action()\n","            if skipped:\n","                continue\n","\n","        next_phi_St, action, reward, done, info = agent.take_e_greedy_action(phi_St)\n","        agent.store_transition(phi_St, action, reward, next_phi_St, done)\n","        phi_St = next_phi_St\n","        batch_size, phi_Sts, actions, rewards, next_phi_Sts, dones = agent.sample_transition()\n","\n","        agent.optimize_loss(batch_size, phi_Sts, actions, rewards, next_phi_Sts, dones)\n","\n","        agent.reset_target_params(epochs)\n","\n","    agent.update_epsilon()\n","    agent.print_status(episode, time.time() - start_time)\n","    agent.save_partial_status(episode)\n","    agent.end_episode()\n","\n","env.close()\n","agent.close()\n","agent.plot()\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Replay Memory loaded. Length:  14000\n","Epsilon loaded. Value:  0.1347935812106403\n","Trained model loaded.\n","\n","episode: 1  score: 0.00000  loss: 1.90207  epsilon: 0.134  24.91s 2020-05-29 13:46:29\n","\n","episode: 2  score: 1.00000  loss: 1.77433  epsilon: 0.134  23.40s 2020-05-29 13:46:52\n","\n","episode: 3  score: 1.00000  loss: 2.42017  epsilon: 0.133  38.35s 2020-05-29 13:47:30\n","\n","episode: 4  score: 2.00000  loss: 2.53537  epsilon: 0.133  29.81s 2020-05-29 13:48:00\n","\n","episode: 5  score: 0.00000  loss: 0.79196  epsilon: 0.132  18.41s 2020-05-29 13:48:19\n","\n","episode: 6  score: 1.00000  loss: 1.09017  epsilon: 0.132  22.41s 2020-05-29 13:48:41\n","\n","episode: 7  score: 0.00000  loss: 0.98255  epsilon: 0.131  18.41s 2020-05-29 13:48:59\n","\n","episode: 8  score: 2.00000  loss: 1.99904  epsilon: 0.131  46.80s 2020-05-29 13:49:46\n","\n","episode: 9  score: 5.00000  loss: 1.92041  epsilon: 0.130  52.94s 2020-05-29 13:50:39\n","\n","episode: 10  score: 3.00000  loss: 2.21113  epsilon: 0.129  54.09s 2020-05-29 13:51:33\n","Mem length:  14000 Size:  116264\n","\n","episode: 11  score: 1.00000  loss: 0.96935  epsilon: 0.129  25.71s 2020-05-29 13:52:00\n","\n","episode: 12  score: 1.00000  loss: 0.93056  epsilon: 0.128  25.61s 2020-05-29 13:52:25\n","\n","episode: 13  score: 1.00000  loss: 1.00834  epsilon: 0.128  25.55s 2020-05-29 13:52:51\n","\n","episode: 14  score: 6.00000  loss: 1.30634  epsilon: 0.127  50.51s 2020-05-29 13:53:41\n","\n","episode: 15  score: 4.00000  loss: 1.33286  epsilon: 0.127  52.73s 2020-05-29 13:54:34\n","\n","episode: 16  score: 0.00000  loss: 0.46176  epsilon: 0.126  18.86s 2020-05-29 13:54:53\n","\n","episode: 17  score: 2.00000  loss: 0.93442  epsilon: 0.126  39.97s 2020-05-29 13:55:33\n","\n","episode: 18  score: 2.00000  loss: 1.87567  epsilon: 0.125  39.40s 2020-05-29 13:56:12\n","\n","episode: 19  score: 1.00000  loss: 2.10889  epsilon: 0.125  29.94s 2020-05-29 13:56:42\n","\n","episode: 20  score: 3.00000  loss: 6.02027  epsilon: 0.124  39.32s 2020-05-29 13:57:21\n","Mem length:  14000 Size:  116264\n","\n","episode: 21  score: 2.00000  loss: 24.70987  epsilon: 0.124  41.46s 2020-05-29 13:58:03\n","\n","episode: 22  score: 1.00000  loss: 30.42805  epsilon: 0.123  45.02s 2020-05-29 13:58:49\n","\n","episode: 23  score: 2.00000  loss: 11.28611  epsilon: 0.123  42.33s 2020-05-29 13:59:31\n","\n","episode: 24  score: 2.00000  loss: 8.08272  epsilon: 0.122  56.91s 2020-05-29 14:00:28\n","\n","episode: 25  score: 3.00000  loss: 7.64721  epsilon: 0.122  43.81s 2020-05-29 14:01:12\n","\n","episode: 26  score: 0.00000  loss: 18.68515  epsilon: 0.121  36.22s 2020-05-29 14:01:48\n","\n","episode: 27  score: 1.00000  loss: 7.66701  epsilon: 0.121  22.72s 2020-05-29 14:02:10\n","\n","episode: 28  score: 2.00000  loss: 3.63247  epsilon: 0.120  39.62s 2020-05-29 14:02:50\n","\n","episode: 29  score: 3.00000  loss: 31.18601  epsilon: 0.120  36.78s 2020-05-29 14:03:27\n","\n","episode: 30  score: 1.00000  loss: 9.01982  epsilon: 0.120  26.10s 2020-05-29 14:03:53\n","Mem length:  14000 Size:  116264\n","\n","episode: 31  score: 4.00000  loss: 4.91148  epsilon: 0.119  43.75s 2020-05-29 14:04:37\n","\n","episode: 32  score: 1.00000  loss: 14.58739  epsilon: 0.119  29.05s 2020-05-29 14:05:06\n","\n","episode: 33  score: 5.00000  loss: 32.54722  epsilon: 0.118  64.75s 2020-05-29 14:06:11\n","\n","episode: 34  score: 3.00000  loss: 42.40449  epsilon: 0.118  43.51s 2020-05-29 14:06:55\n","\n","episode: 35  score: 4.00000  loss: 5.28511  epsilon: 0.117  52.81s 2020-05-29 14:07:47\n","\n","episode: 36  score: 1.00000  loss: 1.98265  epsilon: 0.117  22.60s 2020-05-29 14:08:10\n","\n","episode: 37  score: 1.00000  loss: 5.00373  epsilon: 0.116  28.35s 2020-05-29 14:08:38\n","\n","episode: 38  score: 1.00000  loss: 2.76203  epsilon: 0.116  25.05s 2020-05-29 14:09:03\n","\n","episode: 39  score: 7.00000  loss: 9.37480  epsilon: 0.115  58.06s 2020-05-29 14:10:01\n","\n","episode: 40  score: 7.00000  loss: 11.51299  epsilon: 0.115  60.35s 2020-05-29 14:11:02\n","Mem length:  14000 Size:  116264\n","\n","episode: 41  score: 4.00000  loss: 22.77347  epsilon: 0.114  39.48s 2020-05-29 14:11:42\n","\n","episode: 42  score: 1.00000  loss: 69.28328  epsilon: 0.114  29.19s 2020-05-29 14:12:11\n","\n","episode: 43  score: 2.00000  loss: 119.37621  epsilon: 0.113  32.26s 2020-05-29 14:12:43\n","\n","episode: 44  score: 2.00000  loss: 46.95371  epsilon: 0.113  32.07s 2020-05-29 14:13:15\n","\n","episode: 45  score: 0.00000  loss: 7.35067  epsilon: 0.113  17.61s 2020-05-29 14:13:33\n","\n","episode: 46  score: 0.00000  loss: 1.87023  epsilon: 0.112  17.98s 2020-05-29 14:13:51\n","\n","episode: 47  score: 0.00000  loss: 1.70384  epsilon: 0.112  18.16s 2020-05-29 14:14:09\n","\n","episode: 48  score: 2.00000  loss: 2.47780  epsilon: 0.111  37.01s 2020-05-29 14:14:46\n","\n","episode: 49  score: 0.00000  loss: 0.82473  epsilon: 0.111  18.05s 2020-05-29 14:15:04\n","\n","episode: 50  score: 0.00000  loss: 0.78331  epsilon: 0.110  19.95s 2020-05-29 14:15:24\n","Mem length:  14000 Size:  116264\n","\n","episode: 51  score: 4.00000  loss: 1.56653  epsilon: 0.110  46.29s 2020-05-29 14:16:11\n","\n","episode: 52  score: 11.00000  loss: 1.57258  epsilon: 0.109  48.11s 2020-05-29 14:16:59\n","\n","episode: 53  score: 7.00000  loss: 1.42613  epsilon: 0.109  43.80s 2020-05-29 14:17:43\n","\n","episode: 54  score: 7.00000  loss: 2.30884  epsilon: 0.109  59.31s 2020-05-29 14:18:42\n","\n","episode: 55  score: 3.00000  loss: 1.71543  epsilon: 0.108  41.39s 2020-05-29 14:19:24\n","\n","episode: 56  score: 3.00000  loss: 1.86020  epsilon: 0.108  48.37s 2020-05-29 14:20:12\n","\n","episode: 57  score: 7.00000  loss: 1.35639  epsilon: 0.107  49.88s 2020-05-29 14:21:02\n","\n","episode: 58  score: 7.00000  loss: 1.21013  epsilon: 0.107  46.67s 2020-05-29 14:21:49\n","\n","episode: 59  score: 7.00000  loss: 1.62949  epsilon: 0.106  47.12s 2020-05-29 14:22:36\n","\n","episode: 60  score: 7.00000  loss: 1.55321  epsilon: 0.106  47.60s 2020-05-29 14:23:23\n","Mem length:  14000 Size:  116264\n","\n","episode: 61  score: 8.00000  loss: 1.57158  epsilon: 0.106  71.23s 2020-05-29 14:24:35\n","\n","episode: 62  score: 7.00000  loss: 1.12754  epsilon: 0.105  44.19s 2020-05-29 14:25:19\n","\n","episode: 63  score: 3.00000  loss: 1.33733  epsilon: 0.105  35.69s 2020-05-29 14:25:55\n","\n","episode: 64  score: 7.00000  loss: 1.24621  epsilon: 0.104  49.31s 2020-05-29 14:26:44\n","\n","episode: 65  score: 7.00000  loss: 1.29815  epsilon: 0.104  45.39s 2020-05-29 14:27:30\n","\n","episode: 66  score: 7.00000  loss: 1.24735  epsilon: 0.103  40.90s 2020-05-29 14:28:10\n","\n","episode: 67  score: 3.00000  loss: 1.74980  epsilon: 0.103  56.27s 2020-05-29 14:29:07\n","\n","episode: 68  score: 3.00000  loss: 1.86532  epsilon: 0.103  50.07s 2020-05-29 14:29:57\n","\n","episode: 69  score: 7.00000  loss: 1.13608  epsilon: 0.102  44.57s 2020-05-29 14:30:41\n","\n","episode: 70  score: 7.00000  loss: 0.85058  epsilon: 0.102  44.51s 2020-05-29 14:31:26\n","Mem length:  14000 Size:  116264\n","\n","episode: 71  score: 2.00000  loss: 1.51004  epsilon: 0.101  50.82s 2020-05-29 14:32:17\n","\n","episode: 72  score: 3.00000  loss: 2.28849  epsilon: 0.101  36.70s 2020-05-29 14:32:54\n","\n","episode: 73  score: 3.00000  loss: 1.39164  epsilon: 0.101  55.59s 2020-05-29 14:33:50\n","\n","episode: 74  score: 3.00000  loss: 1.07743  epsilon: 0.100  42.57s 2020-05-29 14:34:32\n","\n","episode: 75  score: 3.00000  loss: 1.53240  epsilon: 0.100  55.86s 2020-05-29 14:35:28\n","\n","episode: 76  score: 8.00000  loss: 1.38734  epsilon: 0.099  50.72s 2020-05-29 14:36:19\n","\n","episode: 77  score: 7.00000  loss: 1.10177  epsilon: 0.099  49.63s 2020-05-29 14:37:08\n","\n","episode: 78  score: 7.00000  loss: 1.30339  epsilon: 0.099  50.86s 2020-05-29 14:37:59\n","\n","episode: 79  score: 2.00000  loss: 1.75047  epsilon: 0.098  62.84s 2020-05-29 14:39:02\n","\n","episode: 80  score: 3.00000  loss: 1.17198  epsilon: 0.098  46.39s 2020-05-29 14:39:48\n","Mem length:  14000 Size:  115736\n","\n","episode: 81  score: 1.00000  loss: 0.95199  epsilon: 0.097  44.29s 2020-05-29 14:40:33\n","\n","episode: 82  score: 3.00000  loss: 1.33155  epsilon: 0.097  52.14s 2020-05-29 14:41:26\n","\n","episode: 83  score: 0.00000  loss: 1.60316  epsilon: 0.097  45.55s 2020-05-29 14:42:11\n","\n","episode: 84  score: 2.00000  loss: 1.34499  epsilon: 0.096  53.67s 2020-05-29 14:43:05\n","\n","episode: 85  score: 3.00000  loss: 2.73215  epsilon: 0.096  38.71s 2020-05-29 14:43:44\n","\n","episode: 86  score: 0.00000  loss: 1.71328  epsilon: 0.095  34.48s 2020-05-29 14:44:18\n","\n","episode: 87  score: 2.00000  loss: 1.69078  epsilon: 0.095  72.39s 2020-05-29 14:45:30\n","\n","episode: 88  score: 3.00000  loss: 1.02051  epsilon: 0.095  57.38s 2020-05-29 14:46:28\n","\n","episode: 89  score: 7.00000  loss: 1.48863  epsilon: 0.094  58.74s 2020-05-29 14:47:26\n","\n","episode: 90  score: 4.00000  loss: 1.50970  epsilon: 0.094  53.25s 2020-05-29 14:48:20\n","Mem length:  14000 Size:  115736\n","\n","episode: 91  score: 2.00000  loss: 1.56858  epsilon: 0.094  45.63s 2020-05-29 14:49:06\n","\n","episode: 92  score: 0.00000  loss: 0.63827  epsilon: 0.093  24.21s 2020-05-29 14:49:30\n","\n","episode: 93  score: 7.00000  loss: 1.73098  epsilon: 0.093  50.68s 2020-05-29 14:50:21\n","\n","episode: 94  score: 2.00000  loss: 1.68282  epsilon: 0.092  59.90s 2020-05-29 14:51:21\n","\n","episode: 95  score: 0.00000  loss: 0.48852  epsilon: 0.092  19.04s 2020-05-29 14:51:40\n","\n","episode: 96  score: 3.00000  loss: 0.70491  epsilon: 0.092  39.77s 2020-05-29 14:52:19\n","\n","episode: 97  score: 0.00000  loss: 0.38919  epsilon: 0.091  22.55s 2020-05-29 14:52:42\n","\n","episode: 98  score: 2.00000  loss: 0.97747  epsilon: 0.091  44.49s 2020-05-29 14:53:27\n","\n","episode: 99  score: 2.00000  loss: 1.04029  epsilon: 0.091  41.40s 2020-05-29 14:54:08\n","\n","episode: 100  score: 1.00000  loss: 0.71324  epsilon: 0.090  23.22s 2020-05-29 14:54:31\n","Mem length:  14000 Size:  116264\n","\n","episode: 101  score: 2.00000  loss: 0.74290  epsilon: 0.090  35.05s 2020-05-29 14:55:07\n","\n","episode: 102  score: 2.00000  loss: 1.22156  epsilon: 0.090  44.90s 2020-05-29 14:55:52\n","\n","episode: 103  score: 2.00000  loss: 1.06993  epsilon: 0.089  41.58s 2020-05-29 14:56:33\n","\n","episode: 104  score: 2.00000  loss: 1.21552  epsilon: 0.089  50.78s 2020-05-29 14:57:24\n","\n","episode: 105  score: 2.00000  loss: 0.93755  epsilon: 0.088  34.77s 2020-05-29 14:57:59\n","\n","episode: 106  score: 2.00000  loss: 3.88614  epsilon: 0.088  41.33s 2020-05-29 14:58:40\n","\n","episode: 107  score: 2.00000  loss: 1.01973  epsilon: 0.088  28.54s 2020-05-29 14:59:09\n","\n","episode: 108  score: 2.00000  loss: 0.68209  epsilon: 0.087  28.27s 2020-05-29 14:59:37\n","\n","episode: 109  score: 1.00000  loss: 1.25790  epsilon: 0.087  49.05s 2020-05-29 15:00:26\n","\n","episode: 110  score: 2.00000  loss: 1.24506  epsilon: 0.087  29.31s 2020-05-29 15:00:55\n","Mem length:  14000 Size:  116264\n","\n","episode: 111  score: 7.00000  loss: 0.96105  epsilon: 0.086  60.68s 2020-05-29 15:01:56\n","\n","episode: 112  score: 9.00000  loss: 0.83233  epsilon: 0.086  56.53s 2020-05-29 15:02:53\n","\n","episode: 113  score: 7.00000  loss: 0.91645  epsilon: 0.086  44.69s 2020-05-29 15:03:38\n","\n","episode: 114  score: 2.00000  loss: 0.66839  epsilon: 0.085  31.75s 2020-05-29 15:04:09\n","\n","episode: 115  score: 1.00000  loss: 0.96859  epsilon: 0.085  38.89s 2020-05-29 15:04:48\n","\n","episode: 116  score: 7.00000  loss: 1.35818  epsilon: 0.085  50.53s 2020-05-29 15:05:39\n","\n","episode: 117  score: 7.00000  loss: 0.79743  epsilon: 0.084  44.99s 2020-05-29 15:06:24\n","\n","episode: 118  score: 3.00000  loss: 0.93448  epsilon: 0.084  36.88s 2020-05-29 15:07:01\n","\n","episode: 119  score: 9.00000  loss: 3.16814  epsilon: 0.084  53.43s 2020-05-29 15:07:54\n","\n","episode: 120  score: 2.00000  loss: 0.79826  epsilon: 0.083  27.76s 2020-05-29 15:08:22\n","Mem length:  14000 Size:  116264\n","\n","episode: 121  score: 2.00000  loss: 1.10051  epsilon: 0.083  33.25s 2020-05-29 15:08:56\n","\n","episode: 122  score: 1.00000  loss: 0.31331  epsilon: 0.083  21.10s 2020-05-29 15:09:17\n","\n","episode: 123  score: 7.00000  loss: 1.01441  epsilon: 0.082  61.12s 2020-05-29 15:10:18\n","\n","episode: 124  score: 7.00000  loss: 1.11407  epsilon: 0.082  65.87s 2020-05-29 15:11:24\n","\n","episode: 125  score: 2.00000  loss: 1.01091  epsilon: 0.082  54.41s 2020-05-29 15:12:18\n","\n","episode: 126  score: 1.00000  loss: 1.09619  epsilon: 0.081  48.00s 2020-05-29 15:13:06\n","\n","episode: 127  score: 2.00000  loss: 0.53987  epsilon: 0.081  29.14s 2020-05-29 15:13:35\n","\n","episode: 128  score: 0.00000  loss: 0.27794  epsilon: 0.081  18.21s 2020-05-29 15:13:53\n","\n","episode: 129  score: 1.00000  loss: 1.32216  epsilon: 0.080  26.63s 2020-05-29 15:14:20\n","\n","episode: 130  score: 6.00000  loss: 4.04617  epsilon: 0.080  80.30s 2020-05-29 15:15:40\n","Mem length:  14000 Size:  116264\n","\n","episode: 131  score: 1.00000  loss: 0.62109  epsilon: 0.080  23.89s 2020-05-29 15:16:05\n","\n","episode: 132  score: 0.00000  loss: 0.45611  epsilon: 0.079  18.26s 2020-05-29 15:16:23\n","\n","episode: 133  score: 1.00000  loss: 0.30061  epsilon: 0.079  22.42s 2020-05-29 15:16:45\n","\n","episode: 134  score: 3.00000  loss: 0.85696  epsilon: 0.079  36.98s 2020-05-29 15:17:22\n","\n","episode: 135  score: 3.00000  loss: 0.48427  epsilon: 0.078  32.36s 2020-05-29 15:17:55\n","\n","episode: 136  score: 3.00000  loss: 1.07364  epsilon: 0.078  31.47s 2020-05-29 15:18:26\n","\n","episode: 137  score: 5.00000  loss: 2.01995  epsilon: 0.078  52.94s 2020-05-29 15:19:19\n","\n","episode: 138  score: 7.00000  loss: 1.49965  epsilon: 0.078  71.16s 2020-05-29 15:20:30\n","\n","episode: 139  score: 2.00000  loss: 0.58953  epsilon: 0.077  30.71s 2020-05-29 15:21:01\n","\n","episode: 140  score: 0.00000  loss: 0.30311  epsilon: 0.077  23.19s 2020-05-29 15:21:24\n","Mem length:  14000 Size:  115736\n","\n","episode: 141  score: 0.00000  loss: 0.52439  epsilon: 0.077  18.60s 2020-05-29 15:21:43\n","\n","episode: 142  score: 3.00000  loss: 1.06462  epsilon: 0.076  41.42s 2020-05-29 15:22:25\n","\n","episode: 143  score: 7.00000  loss: 1.26004  epsilon: 0.076  56.60s 2020-05-29 15:23:21\n","\n","episode: 144  score: 1.00000  loss: 0.89114  epsilon: 0.076  30.09s 2020-05-29 15:23:52\n","\n","episode: 145  score: 2.00000  loss: 0.89187  epsilon: 0.075  31.33s 2020-05-29 15:24:23\n","\n","episode: 146  score: 3.00000  loss: 1.00208  epsilon: 0.075  35.55s 2020-05-29 15:24:58\n","\n","episode: 147  score: 1.00000  loss: 0.60488  epsilon: 0.075  21.91s 2020-05-29 15:25:20\n","\n","episode: 148  score: 4.00000  loss: 0.89360  epsilon: 0.074  38.97s 2020-05-29 15:25:59\n","\n","episode: 149  score: 2.00000  loss: 0.78374  epsilon: 0.074  32.11s 2020-05-29 15:26:31\n","\n","episode: 150  score: 1.00000  loss: 0.44342  epsilon: 0.074  23.38s 2020-05-29 15:26:55\n","Mem length:  14000 Size:  116264\n","\n","episode: 151  score: 3.00000  loss: 0.70529  epsilon: 0.074  35.76s 2020-05-29 15:27:31\n","\n","episode: 152  score: 3.00000  loss: 1.03619  epsilon: 0.073  40.56s 2020-05-29 15:28:12\n","\n","episode: 153  score: 7.00000  loss: 1.48610  epsilon: 0.073  42.24s 2020-05-29 15:28:54\n","\n","episode: 154  score: 2.00000  loss: 1.54326  epsilon: 0.073  36.89s 2020-05-29 15:29:31\n","\n","episode: 155  score: 3.00000  loss: 0.73738  epsilon: 0.072  43.51s 2020-05-29 15:30:14\n","\n","episode: 156  score: 1.00000  loss: 1.06752  epsilon: 0.072  45.93s 2020-05-29 15:31:00\n","\n","episode: 157  score: 1.00000  loss: 1.18188  epsilon: 0.072  30.95s 2020-05-29 15:31:31\n","\n","episode: 158  score: 2.00000  loss: 1.29833  epsilon: 0.072  42.17s 2020-05-29 15:32:13\n","\n","episode: 159  score: 3.00000  loss: 2.26157  epsilon: 0.071  78.50s 2020-05-29 15:33:32\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-2454a17988a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_Sts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_phi_Sts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_Sts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_phi_Sts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_target_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-74c11860aa56>\u001b[0m in \u001b[0;36moptimize_loss\u001b[0;34m(self, batch_size, phi_Sts, actions, rewards, next_phi_Sts, dones)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m#print(\"prediction next states \", self.model.predict(next_phi_Sts))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_Sts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepi_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \"\"\"\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m       return ParallelMapDataset(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3980\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3981\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3982\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3983\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3219\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \"\"\"\n\u001b[1;32m   2531\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2532\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2496\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2497\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3212\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3213\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3214\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3215\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3154\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3156\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3157\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3158\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mpermutation\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    317\u001b[0m       \u001b[0;31m# than reusing the same range Tensor. (presumably because of buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;31m# forwarding.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m       \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[0;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[1;32m   1571\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"limit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m       \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;31m# infer dtype if not explicitly provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    304\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[1;32m    305\u001b[0m              \"dtype\": dtype_value},\n\u001b[0;32m--> 306\u001b[0;31m       name=name).outputs[0]\n\u001b[0m\u001b[1;32m    307\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3325\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3328\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 1817\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   1818\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"iov6lxes-jR8","colab_type":"code","colab":{}},"source":["\n","with open(agent.datapath + \"train_score.file\", \"wb\") as f:\n","    pickle.dump(agent.total_score, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDLRpK0-c7ts","colab_type":"code","colab":{}},"source":["with open(agent.replay_mem_file, \"wb\") as f:\n","  pickle.dump(agent.replay_mem, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-peWrD9P-vJu","colab_type":"code","colab":{}},"source":["def moving_avg(list_, n):\n","    cumsum, moving_aves = [0], []\n","    for i, x in enumerate(list_, 1):\n","        cumsum.append(cumsum[i-1] + x)\n","        if i>=n:\n","            moving_ave = (cumsum[i] - cumsum[i-n])/float(n)\n","            moving_aves.append(moving_ave)\n","    return moving_aves"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9YtB69n5-5VG","colab_type":"code","colab":{}},"source":["n = 10\n","dqn_score = agent.datapath + \"train_score_1_500.file\"\n","dqn_score1 = agent.datapath + \"train_score.file\"\n","graph_file = agent.datapath + \"avg_score_10.png\"\n","\n","dqn_score = pickle.load(open(dqn_score, \"rb\"))\n","dqn_score1 = pickle.load(open(dqn_score1, \"rb\"))\n","dqn_score.extend(dqn_score1)\n","dqn_score1 = moving_avg(dqn_score, n)\n","\n","if len(dqn_score) > 1:\n","    N = np.arange(0, len(dqn_score))\n","    plt.figure()\n","    plt.plot(N, dqn_score, label = \"dqn_reward\")\n","    plt.title(\"\")\n","    plt.xlabel(\"Episode #\")\n","    plt.ylabel(\"Reward\")\n","    plt.legend()\n","    plt.savefig(agent.datapath + \"score.png\")\n","    plt.close()\n","\n","    N = np.arange(0, len(dqn_score1))\n","    plt.figure()\n","    plt.plot(N, dqn_score1, label = \"dqn_reward\")\n","    plt.title(\"\")\n","    plt.xlabel(\"Episode #\")\n","    plt.ylabel(\"Reward\")\n","    plt.legend()\n","    plt.savefig(graph_file)\n","    plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oX0e_-hAT6Wp","colab_type":"text"},"source":["EVALUATE"]},{"cell_type":"code","metadata":{"id":"5kJ5GF-pT5qH","colab_type":"code","outputId":"a2f3870f-9c9e-4de8-b745-b923978ddc9d","executionInfo":{"status":"ok","timestamp":1590723490917,"user_tz":-330,"elapsed":36543,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#agent = DQNAgent()\n","agent = tfk.models.load_model(base_dir + \"data/model1/trained_model.h5\")\n","\n","epochs = 0\n","env = wrap_env(gym.make('BreakoutDeterministic-v0')) #gym.make ('CarRacing-v0') \n","\n","no_of_episodes = 10\n","input_shape = (80, 80, 4)\n","\n","for i in range(no_of_episodes):\n","    done = False\n","    phi_St = preprocess(np.zeros(input_shape), env.reset())\n","    env.render()\n","    R = 0\n","    while not done:\n","        epochs += 1\n","        phi_St = phi_St.reshape((1, 80, 80, 4))\n","        actions = agent.predict(phi_St)\n","        print(actions)\n","        action = np.argmax(actions)\n","        next_x, reward, done, info = env.step(action)\n","        env.render()\n","        phi_St = preprocess(np.squeeze(phi_St, axis=0), next_x)\n","\n","        R += reward\n","    print(\"Reward: \", R) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[330.98224 351.0099  338.39154 334.3865 ]]\n","[[295.795   315.27682 302.6391  298.85474]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[330.98224 351.0099  338.39154 334.3865 ]]\n","[[295.795   315.27682 302.6391  298.85474]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n","[[282.62753 303.73767 289.52042 285.58115]]\n","[[348.74066 366.84116 355.5993  352.91125]]\n","[[264.07202 283.79483 270.51065 266.8302 ]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[216.52701 232.45088 221.76698 218.78134]]\n","[[256.6223  271.78976 262.31055 259.2521 ]]\n","[[333.62628 344.8734  339.82483 336.9463 ]]\n","[[340.58096 350.58435 346.69922 343.95218]]\n","[[286.12405 295.1934  291.35486 288.96088]]\n","[[301.43295 313.35364 307.28098 304.45233]]\n","[[347.98053 357.36603 354.11356 351.4151 ]]\n","[[368.25302 377.34567 374.62546 371.87854]]\n","[[203.65666 218.86224 208.61621 205.77846]]\n","[[217.97536 234.25154 223.28545 220.24799]]\n","[[267.25552 278.47833 272.53073 269.93808]]\n","[[283.69382 297.03442 289.49774 286.56067]]\n","[[226.62006 241.30443 231.82384 228.9558 ]]\n","[[204.89243 220.19041 209.88223 207.02725]]\n","[[226.413   242.42915 231.80309 228.76337]]\n","[[245.08603 260.24985 250.61385 247.60503]]\n","[[157.21434 168.94746 161.03687 158.84692]]\n","[[ 99.28889  105.99634  101.490585 100.11389 ]]\n","[[92.98789 99.89549 95.2312  93.9373 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[141.21855 151.75569 144.64949 142.68266]]\n","[[125.46013 134.81903 128.50526 126.75823]]\n","[[133.07178 142.99979 136.30325 134.45006]]\n","[[147.62718 157.63278 150.92009 148.87155]]\n","[[158.05263 169.02063 161.65414 159.4588 ]]\n","[[138.08623 148.38916 141.44046 139.51733]]\n","[[168.83177 181.43353 172.93874 170.58675]]\n","[[176.60541 189.78839 180.90268 178.44226]]\n","[[158.74371 170.5912  162.6037  160.39243]]\n","[[203.56755 218.76646 208.52492 205.68842]]\n","[[199.92499 214.85155 204.79317 202.00748]]\n","[[217.46294 231.49777 222.11768 219.10406]]\n","[[181.09167 194.61009 185.49878 182.97578]]\n","[[164.99084 177.3054  169.00375 166.70535]]\n","[[152.24716 163.60889 155.94809 153.82742]]\n","[[134.54019 142.34933 137.15733 135.30057]]\n","[[131.59698 138.44736 133.92653 132.11642]]\n","[[118.77656 124.89592 120.85861 119.22561]]\n","[[108.380844 115.42193  110.70413  109.20324 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[228.37599 238.33363 232.05476 228.62915]]\n","[[199.96236 206.92279 202.50623 199.7805 ]]\n","[[175.71182 182.32794 178.09058 175.69197]]\n","[[173.75266 181.34401 176.41075 174.03096]]\n","[[144.96927 155.01657 148.26726 146.25397]]\n","[[155.48767 167.09169 159.26793 157.10207]]\n","[[184.97421 198.78291 189.47636 186.89922]]\n","[[169.65523 182.31854 173.78233 171.41887]]\n","[[165.56383 177.92125 169.59079 167.28438]]\n","[[176.35619 189.52052 180.64735 178.1904 ]]\n","[[190.0133  204.19879 194.63884 191.9914 ]]\n","[[203.63737 218.84152 208.59645 205.75897]]\n","[[185.12904 198.94934 189.635   187.05568]]\n","[[168.26054 180.81958 172.35352 170.00949]]\n","[[190.42165 204.63766 195.05717 192.40404]]\n","[[195.69833 209.07222 200.1022  197.38484]]\n","[[149.84953 159.35857 153.00348 150.92896]]\n","[[140.23573 148.36539 142.96156 141.02617]]\n","[[129.8208  136.91626 132.21712 130.42892]]\n","[[116.53074  122.83031  118.659386 117.05507 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[252.22615 267.4586  257.86267 254.81454]]\n","[[250.40251 265.5287  255.99867 252.97208]]\n","[[303.43198 317.28268 309.58194 306.49466]]\n","[[305.07883 316.57483 310.91708 308.1279 ]]\n","[[318.66592 332.4384  325.0159  321.87387]]\n","[[349.9464  360.41318 356.2603  353.41336]]\n","[[367.99576 379.98926 374.77673 371.65506]]\n","[[380.36905 388.33072 386.74872 384.09674]]\n","[[269.3271  282.1892  274.8639  272.05002]]\n","[[294.1326  302.49924 299.37457 297.03754]]\n","[[273.89996 285.96088 279.38632 276.6568 ]]\n","[[247.64674 259.28046 252.70856 250.14621]]\n","[[194.0812  208.57083 198.80632 196.10214]]\n","[[181.38734 194.92787 185.80168 183.27457]]\n","[[180.46657 193.93825 184.85837 182.34409]]\n","[[186.37437 200.28778 190.91081 188.31413]]\n","[[114.78713 123.34803 117.57096 115.97281]]\n","[[107.97338  116.02481  110.590385 109.08728 ]]\n","[[103.2774   110.97772  105.77944  104.341835]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","[[83.459885 88.25606  85.061714 83.91132 ]]\n","Reward:  0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PN-9B3CBCY1S","colab_type":"code","colab":{}},"source":["show_video()"],"execution_count":0,"outputs":[]}]}